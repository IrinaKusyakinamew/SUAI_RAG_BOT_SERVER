import json
import re
import uuid
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
import shutil
import tiktoken

# –ü–∞–ø–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏
TMP_CHUNKS_DIR = Path(__file__).parent / "tmp_chunks"

# –ü–∞–ø–∫–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö JSON –ø–µ—Ä–µ–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
TMP_JSON_DIR = Path(__file__).parent / "tmp_json_for_embedding"
TMP_JSON_DIR.mkdir(parents=True, exist_ok=True)

# –ò—Ç–æ–≥–æ–≤—ã–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π JSON
OUTPUT_JSON = TMP_JSON_DIR / "all_chunks.json"

# –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ (–¥–ª—è html/pdf/docx)
MIN_TOKENS = 50

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –¥–ª—è GPT —Ç–æ–∫–µ–Ω–æ–≤
ENC = tiktoken.encoding_for_model("gpt-3.5-turbo")


def clean_text(text: str) -> str:
    text = text.replace("\xa0", " ").replace("\u200b", " ")
    text = re.sub(r"[\r\n\t]+", " ", text)
    text = re.sub(r"\s{2,}", " ", text)
    text = re.sub(r"\s+([,.!?;:])", r"\1", text)
    return text.strip()


def tokenize(text: str) -> int:
    """–ü–æ–¥—Å—á—ë—Ç GPT-—Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ tiktoken"""
    return len(ENC.encode(text))


def looks_like_navigation(html_fragment: str) -> bool:
    """–ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è HTML –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤"""
    soup = BeautifulSoup(html_fragment, "lxml")
    text = soup.get_text(separator=" ", strip=True)
    if len(text.split()) < 5:
        return True
    nav_tags = soup.find_all(["nav", "header", "footer", "aside", "menu"])
    if nav_tags:
        return True
    link_ratio = len(soup.find_all("a")) / (len(text.split()) + 1)
    if link_ratio > 0.5:
        return True
    return False


def normalize_chunk(chunk: dict, source_url: str, doc_type: str, chunk_id: int, offset: int):
    """–°–æ–∑–¥–∞—ë—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç —á–∞–Ω–∫–∞ —Å UUID –∏ GPT-—Ç–æ–∫–µ–Ω–∞–º–∏"""
    text = clean_text(chunk.get("text", ""))
    if not text:
        return None, offset

    token_count = tokenize(text)
    start_offset = offset
    end_offset = offset + len(text)
    offset = end_offset

    chunk_uid = str(uuid.uuid4())

    normalized = {
        "chunk_id": chunk_id,
        "chunk_uid": chunk_uid,
        "text": text,
        "token_count": token_count,
        "start_offset": start_offset,
        "end_offset": end_offset,
        "document_id": source_url,
        "source_url": source_url,
        "type": doc_type,
        "metadata": {
            "source": doc_type,
            "path": source_url,
            "created_at": datetime.utcnow().isoformat() + "Z"
        }
    }
    return normalized, offset


def main():
    all_chunks = []
    global_chunk_id = 0
    stats = {"kept": 0, "removed_short": 0, "removed_structural": 0, "removed_empty": 0}

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ JSON —Ñ–∞–π–ª—ã –∏–∑ tmp_chunks
    input_files = sorted(TMP_CHUNKS_DIR.glob("*.json"))
    if not input_files:
        print("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ tmp_chunks")
        return

    for file_path in input_files:
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {file_path.name}...")
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        chunks = data.get("chunks", data if isinstance(data, list) else [])
        offset = 0
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_type = (
            "html" if "html" in file_path.name
            else "pdf" if "pdf" in file_path.name
            else "docx" if "docx" in file_path.name
            else "txt"
        )

        for ch in chunks:
            text = clean_text(ch.get("text", ""))
            if not text:
                stats["removed_empty"] += 1
                continue

            token_count = tokenize(text)

            # –î–ª—è html/pdf/docx –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            if doc_type != "txt":
                if token_count < MIN_TOKENS:
                    stats["removed_short"] += 1
                    continue

                if doc_type == "html":
                    raw_html = ch.get("raw_html", ch.get("text", ""))
                    if looks_like_navigation(raw_html):
                        stats["removed_structural"] += 1
                        continue

            # –î–ª—è txt —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            source_url = (
                ch.get("source_url")
                or ch.get("document_id")
                or ch.get("filename")
                or file_path.name
            )

            normalized, offset = normalize_chunk(ch, source_url, doc_type, global_chunk_id, offset)
            if normalized:
                all_chunks.append(normalized)
                global_chunk_id += 1
                stats["kept"] += 1

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π JSON
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump({"chunks": all_chunks}, f, ensure_ascii=False, indent=2)

    print("\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —á–∞–Ω–∫–æ–≤:      {stats['kept']}")
    print(f"  –£–¥–∞–ª–µ–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö:      {stats['removed_short']}")
    print(f"  –£–¥–∞–ª–µ–Ω–æ –ø—É—Å—Ç—ã—Ö:        {stats['removed_empty']}")
    print(f"  –£–¥–∞–ª–µ–Ω–æ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {stats['removed_structural']}")
    print(f"  ‚ûú –ò—Ç–æ–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {OUTPUT_JSON}")

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ JSON
    if TMP_CHUNKS_DIR.exists():
        shutil.rmtree(TMP_CHUNKS_DIR)
        print(f"üóë –ü–∞–ø–∫–∞ {TMP_CHUNKS_DIR} —É–¥–∞–ª–µ–Ω–∞")


if __name__ == "__main__":
    main()
