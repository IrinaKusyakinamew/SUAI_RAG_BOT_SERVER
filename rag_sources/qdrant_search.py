from qdrant_client import QdrantClient, models
from qdrant_client.models import Filter, FieldCondition, MatchAny, MatchText
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Set, Optional
import re
import json
import os
import requests

QDRANT_URL = "http://212.192.220.24:6333"
API_KEY = "pii5z%cE1"
COLLECTION = "schedules_embeddings"

DOC_PREFIX = {
    "group": "groups_",  # –¥–ª—è –≥—Ä—É–ø–ø
    "room": "classrooms_",  # –¥–ª—è –∞—É–¥–∏—Ç–æ—Ä–∏–π
    "teacher": "teachers_",  # –¥–ª—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π
}


def extract_metadata(payload: dict) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç metadata –∏–∑ payload"""
    metadata = payload.get("metadata", {})
    if isinstance(metadata, str):
        try:
            metadata = json.loads(metadata)
        except:
            metadata = {}
    result = payload.copy()
    result.update(metadata)
    if "metadata" in result:
        del result["metadata"]
    return result


class UniversityBot:
    def __init__(self, qdrant_url: str, api_key: str, llm_api_key: str = None):
        embed_model = "../model"
        print("Loading embedding model...")
        self.model = SentenceTransformer(embed_model, device="cpu")
        print("Model loaded")
        self.qdrant = QdrantClient(url=qdrant_url, api_key=api_key, timeout=30, prefer_grpc=False)
        print("Qdrant client initialized")
        self.text_collection = "text_embeddings"
        self.schedule_collection = "schedules_embeddings"

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –¥–ª—è –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        if llm_api_key:
            self.llm = LLMGenerator(
                provider="caila",
                api_key=llm_api_key,
                model="gpt-4o-mini",
                temperature=0.1
            )
            self.has_llm = True
        else:
            self.has_llm = False
            print("LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")

    # –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
    def search_schedule(self, search_type: str, search_value: str, limit=1000):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if search_type == 'group':
            filter_key = "metadata.groups"
            match_type = MatchAny(any=[search_value])
            doc_prefix = DOC_PREFIX["group"]

        elif search_type == 'room':
            filter_key = "metadata.room"
            clean_value = re.sub(r'^(–∞—É–¥\.?|–∞—É–¥–∏—Ç–æ—Ä–∏—è|–∞—É–¥)\s*', '', search_value.lower()).strip()

            search_patterns = [clean_value]
            if '-' in clean_value:
                search_patterns.append(clean_value.replace('-', ''))
            search_patterns.append(f" {clean_value} ")

            room_conditions = []
            for pattern in search_patterns:
                room_conditions.append(
                    FieldCondition(key=filter_key, match=MatchText(text=pattern))
                )

            match_type = Filter(should=room_conditions)
            doc_prefix = DOC_PREFIX["room"]

        elif search_type == 'teacher':
            filter_key = "metadata.teacher"
            match_type = MatchText(text=search_value)
            doc_prefix = DOC_PREFIX["teacher"]

        else:
            return []

        try:
            if search_type == 'room':
                filter_ = match_type
            else:
                filter_ = Filter(must=[FieldCondition(key=filter_key, match=match_type)])

            all_points = []
            next_offset = None
            total_scanned = 0

            while total_scanned < limit:
                try:
                    scroll_result = self.qdrant.scroll(
                        collection_name=self.schedule_collection,
                        scroll_filter=filter_,
                        limit=500,
                        offset=next_offset,
                        with_payload=True
                    )

                    if not scroll_result:
                        break

                    points, next_offset = scroll_result

                    if not points:
                        break

                    all_points.extend(points)
                    total_scanned += len(points)

                    if next_offset is None or len(points) == 0:
                        break

                except Exception:
                    break

            points = all_points

            filtered_points = []
            for point in points:
                payload = point.payload or {}
                doc_id = payload.get('document_id', '')

                if doc_id.startswith(doc_prefix):
                    filtered_points.append(point)

            unique_lessons = set()
            lessons = []

            for point in filtered_points:
                payload = point.payload or {}
                metadata = extract_metadata(payload)

                day = metadata.get("day", "")
                time = metadata.get("time", "")
                subject = metadata.get("subject", "")
                week = metadata.get("week", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ")
                room = metadata.get("room", "")
                teacher = metadata.get("teacher", [])
                groups = metadata.get("groups", [])

                key = f"{day}|{time}|{subject}|{week}"

                if key not in unique_lessons:
                    unique_lessons.add(key)
                    lessons.append({
                        "day": day,
                        "time": time,
                        "subject": subject,
                        "week": week,
                        "room": room,
                        "teacher": teacher,
                        "groups": groups,
                        "score": 1.0,
                        "payload": metadata
                    })

            day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                         "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°–∞–±–±–æ—Ç–∞": 6}
            time_order = {"1 –ø–∞—Ä–∞": 1, "2 –ø–∞—Ä–∞": 2, "3 –ø–∞—Ä–∞": 3,
                          "4 –ø–∞—Ä–∞": 4, "5 –ø–∞—Ä–∞": 5, "6 –ø–∞—Ä–∞": 6}

            lessons.sort(key=lambda x: (day_order.get(x["day"], 99), time_order.get(x["time"], 99)))

            return lessons

        except Exception:
            return []

    def _extract_metadata(self, payload: Dict) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ payload"""
        return extract_metadata(payload)

    def detect_query_type(self, query: str) -> Dict[str, Any]:
        original_query = query
        query_lower = query.lower().strip()

        analysis = {
            "type": "general",
            "is_schedule": False,
            "groups": [],
            "rooms": [],
            "teachers": [],
            "days": [],
            "times": [],
            "original_query": original_query,
        }

        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        schedule_keywords = ["—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ", "–ø–∞—Ä–∞", "–ø–∞—Ä—ã", "–∞—É–¥–∏—Ç–æ—Ä–∏—è", "–∞—É–¥",
                             "–ª–µ–∫—Ü–∏—è", "–∑–∞–Ω—è—Ç–∏–µ", "—Å–µ–º–∏–Ω–∞—Ä", "–ø—Ä–∞–∫—Ç–∏–∫–∞"]
        has_schedule_kw = any(kw in query_lower for kw in schedule_keywords)

        # 2. –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        # –ì—Ä—É–ø–ø—ã
        group_matches = re.findall(r'\b\d{3,4}[–∞-—è–º–∫]?\b', query_lower)
        if group_matches:
            analysis["groups"] = list(set(group_matches))

        # –ê—É–¥–∏—Ç–æ—Ä–∏–∏ (–Ω–æ–º–µ—Ä–∞ —Å –¥–µ—Ñ–∏—Å–æ–º)
        room_matches = re.findall(r'\b\d+-\d+\b', query)
        if room_matches:
            analysis["rooms"] = room_matches

        # –î–Ω–∏ –Ω–µ–¥–µ–ª–∏
        days = ["–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫", "—Å—Ä–µ–¥–∞", "—á–µ—Ç–≤–µ—Ä–≥", "–ø—è—Ç–Ω–∏—Ü–∞", "—Å—É–±–±–æ—Ç–∞"]
        for day in days:
            if day in query_lower:
                analysis["days"].append(day.capitalize())

        # –ü–∞—Ä—ã
        for i in range(1, 7):
            if f"{i} –ø–∞—Ä–∞" in query_lower or f"{i}-—è –ø–∞—Ä–∞" in query_lower:
                analysis["times"].append(f"{i} –ø–∞—Ä–∞")

        # 3. –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏ - –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        if has_schedule_kw or analysis["groups"] or analysis["rooms"] or analysis["days"] or analysis["times"]:
            # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –¥–ª–∏–Ω–Ω–µ–µ 2 –±—É–∫–≤
            words = original_query.split()
            for word in words:
                clean_word = re.sub(r'[.,!?;:]', '', word)
                if (len(clean_word) > 2 and
                        clean_word[0].isupper() and
                        not clean_word.isdigit() and
                        clean_word.lower() not in days and
                        clean_word.lower() not in schedule_keywords):
                    analysis["teachers"].append(clean_word)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        analysis["teachers"] = list(set(analysis["teachers"]))

        # 4. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        has_schedule_entities = any([
            analysis["groups"],
            analysis["rooms"],
            analysis["days"],
            analysis["times"],
            analysis["teachers"],  # –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏ —Ç–æ–∂–µ —Å—á–∏—Ç–∞–µ–º —Å—É—â–Ω–æ—Å—Ç—å—é —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        ])

        if has_schedule_kw or has_schedule_entities:
            analysis["is_schedule"] = True
            analysis["type"] = "schedule"
        else:
            analysis["type"] = "general"

        return analysis

    def search_schedule_combined(self, criteria: Dict[str, Any], limit=1000):
        """ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º """
        filters = []

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è
        if criteria.get("groups"):
            for group in criteria["groups"]:
                filters.append(
                    FieldCondition(key="metadata.groups", match=MatchAny(any=[group]))
                )

        if criteria.get("rooms"):
            for room in criteria["rooms"]:
                clean_value = re.sub(r'^(–∞—É–¥\.?|–∞—É–¥–∏—Ç–æ—Ä–∏—è|–∞—É–¥)\s*', '', room.lower()).strip()
                search_patterns = [clean_value]
                if '-' in clean_value:
                    search_patterns.append(clean_value.replace('-', ''))
                search_patterns.append(f" {clean_value} ")

                room_conditions = []
                for pattern in search_patterns:
                    room_conditions.append(
                        FieldCondition(key="metadata.room", match=MatchText(text=pattern))
                    )

                if room_conditions:
                    if len(room_conditions) > 1:
                        filters.append(Filter(should=room_conditions))
                    else:
                        filters.append(room_conditions[0])

        if criteria.get("teachers"):
            for teacher in criteria["teachers"]:
                filters.append(
                    FieldCondition(key="metadata.teacher", match=MatchText(text=teacher))
                )

        if criteria.get("days"):
            for day in criteria["days"]:
                filters.append(
                    FieldCondition(key="metadata.day", match=MatchText(text=day))
                )

        if criteria.get("times"):
            for time in criteria["times"]:
                filters.append(
                    FieldCondition(key="metadata.time", match=MatchText(text=time))
                )

        if not filters:
            return []

        filter_ = Filter(must=filters)

        try:
            all_points = []
            next_offset = None
            total_scanned = 0

            while total_scanned < limit:
                try:
                    scroll_result = self.qdrant.scroll(
                        collection_name=self.schedule_collection,
                        scroll_filter=filter_,
                        limit=500,
                        offset=next_offset,
                        with_payload=True
                    )

                    if not scroll_result or not scroll_result[0]:
                        break

                    points, next_offset = scroll_result
                    all_points.extend(points)
                    total_scanned += len(points)

                    if next_offset is None:
                        break

                except Exception:
                    break

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            lessons = []
            seen_keys = set()

            for point in all_points:
                payload = point.payload or {}
                doc_id = payload.get('document_id', '')

                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
                if not doc_id.startswith(('groups_', 'classrooms_', 'teachers_')):
                    continue

                metadata = extract_metadata(payload)

                day = metadata.get("day", "")
                time = metadata.get("time", "")
                subject = metadata.get("subject", "")
                week = metadata.get("week", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ")

                key = f"{day}|{time}|{subject}|{week}"

                if key not in seen_keys:
                    seen_keys.add(key)
                    lessons.append({
                        "day": day,
                        "time": time,
                        "subject": subject,
                        "week": week,
                        "room": metadata.get("room", ""),
                        "teacher": metadata.get("teacher", []),
                        "groups": metadata.get("groups", []),
                    })

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                         "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°—É–±–±–æ—Ç–∞": 6}
            time_order = {"1 –ø–∞—Ä–∞": 1, "2 –ø–∞—Ä–∞": 2, "3 –ø–∞—Ä–∞": 3,
                          "4 –ø–∞—Ä–∞": 4, "5 –ø–∞—Ä–∞": 5, "6 –ø–∞—Ä–∞": 6}

            lessons.sort(key=lambda x: (day_order.get(x["day"], 99), time_order.get(x["time"], 99)))

            return lessons

        except Exception:
            return []

    def search_schedule_flexible(self, query: str, criteria: Dict[str, Any], limit=1000):
        """ –ì–∏–±–∫–∏–π –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É """
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        if any([criteria["groups"], criteria["rooms"], criteria["teachers"],
                criteria["days"], criteria["times"]]):
            return self.search_schedule_combined(criteria, limit)

        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –Ω–æ –∑–∞–ø—Ä–æ—Å —è–≤–Ω–æ –æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        query_vector = self.model.encode(query, normalize_embeddings=True).tolist()

        try:
            results = self.qdrant.search(
                collection_name=self.schedule_collection,
                query_vector=query_vector,
                limit=limit,
                with_payload=True,
            )

            lessons = []
            seen_keys = set()

            for point in results:
                payload = point.payload or {}
                doc_id = payload.get('document_id', '')

                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–∑ —Ñ–∞–π–ª–æ–≤ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
                if not doc_id.startswith(('groups_', 'classrooms_', 'teachers_')):
                    continue

                metadata = extract_metadata(payload)

                day = metadata.get("day", "")
                time = metadata.get("time", "")
                subject = metadata.get("subject", "")
                week = metadata.get("week", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ")

                key = f"{day}|{time}|{subject}|{week}"

                if key not in seen_keys:
                    seen_keys.add(key)
                    lessons.append({
                        "day": day,
                        "time": time,
                        "subject": subject,
                        "week": week,
                        "room": metadata.get("room", ""),
                        "teacher": metadata.get("teacher", []),
                        "groups": metadata.get("groups", []),
                        "score": float(point.score),  # –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    })

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å–Ω–∞—á–∞–ª–∞ –ø–æ –¥–Ω—é/–≤—Ä–µ–º–µ–Ω–∏, –ø–æ—Ç–æ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                         "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°—É–±–±–æ—Ç–∞": 6}
            time_order = {"1 –ø–∞—Ä–∞": 1, "2 –ø–∞—Ä–∞": 2, "3 –ø–∞—Ä–∞": 3,
                          "4 –ø–∞—Ä–∞": 4, "5 –ø–∞—Ä–∞": 5, "6 –ø–∞—Ä–∞": 6}

            lessons.sort(key=lambda x: (
                day_order.get(x["day"], 99),
                time_order.get(x["time"], 99),
                -x.get("score", 0)  # –ü–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            ))

            return lessons

        except Exception:
            return []

    # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    def search_documents(self, query: str, collection_name: str, top_k: int = 50) -> List[Dict]:
        query_vector = self.model.encode(query, normalize_embeddings=True).tolist()
        try:
            points = self.qdrant.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=top_k,
                with_payload=True
            )
            print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–µ–∫: {len(points)}")
            results = []
            for point in points:
                payload = point.payload or {}
                full_payload = self._extract_metadata(payload)
                doc_type = "schedule" if full_payload.get("source") == "schedule" else "general"
                results.append({
                    "id": point.id,
                    "score": float(point.score),
                    "text": full_payload.get("full_text", ""),
                    "type": doc_type,
                    "payload": full_payload
                })
            return results
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}': {e}")
            return []

    def _get_collection_for_query(self, query_type: str) -> str:
        return self.schedule_collection if query_type == "schedule" else self.text_collection

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
    def filter_schedule_results(self, results: List[Dict], analysis: Dict[str, Any]) -> List[Dict]:
        filtered = []
        for result in results:
            if result.get("type") != "schedule":
                continue
            chunk = result.get("payload")
            if not chunk:
                continue

            # –§–∏–ª—å—Ç—Ä—ã
            if analysis["groups"]:
                groups_from_chunk = chunk.get("groups", [])
                chunk_groups_clean = []
                for group_item in groups_from_chunk:
                    if isinstance(group_item, str):
                        match = re.search(r'(\d{3,4}[–∞-—è–º–∫]?)', group_item)
                        if match:
                            chunk_groups_clean.append(match.group(1).lower())
                if chunk_groups_clean and not any(g.lower() in chunk_groups_clean for g in analysis["groups"]):
                    continue

            if analysis["teachers"]:
                teachers_from_chunk = chunk.get("teacher", [])
                if isinstance(teachers_from_chunk, list):
                    chunk_teachers_clean = [item.lower() for i, item in enumerate(teachers_from_chunk) if
                                            i % 2 == 0 and isinstance(item, str)]
                    if chunk_teachers_clean and not any(
                            t.lower() in chunk_teachers_clean for t in analysis["teachers"]):
                        continue

            if analysis["rooms"]:
                room_str = str(chunk.get("room", "")).lower()
                if room_str and not any(r in room_str for r in analysis["rooms"]):
                    continue

            if analysis["days"]:
                day_str = str(chunk.get("day", "")).lower()
                if day_str and not any(d.lower() in day_str for d in analysis["days"]):
                    continue

            filtered.append(result)
        return filtered

    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å search_schedule)
    def format_schedule_from_lessons(self, lessons: List[Dict]) -> str:
        if not lessons:
            return " –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                     "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°—É–±–±–æ—Ç–∞": 6, "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": 7}

        lessons_by_day = {}
        for lesson in lessons:
            day = lesson.get("day", "")
            if day not in lessons_by_day:
                lessons_by_day[day] = []
            lessons_by_day[day].append(lesson)

        output = ["üìÖ **–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ**"]

        for day in sorted(lessons_by_day.keys(), key=lambda x: day_order.get(x, 99)):
            output.append(f"\nüìÜ {day}:")
            day_lessons = lessons_by_day[day]

            time_order = {"1 –ø–∞—Ä–∞": 1, "2 –ø–∞—Ä–∞": 2, "3 –ø–∞—Ä–∞": 3,
                          "4 –ø–∞—Ä–∞": 4, "5 –ø–∞—Ä–∞": 5, "6 –ø–∞—Ä–∞": 6}
            day_lessons.sort(key=lambda x: time_order.get(x.get("time", ""), 99))

            for i, lesson in enumerate(day_lessons, 1):
                output.append(f"\n{i}. **{lesson.get('time', '')}**")
                output.append(f"   üìö {lesson.get('subject', '')}")

                room = lesson.get('room', '')
                if room and room != '–Ω–µ —É–∫–∞–∑–∞–Ω–æ':
                    output.append(f"   üè¢ –ê—É–¥–∏—Ç–æ—Ä–∏—è: {room}")

                teachers = lesson.get('teacher', [])
                if isinstance(teachers, list) and teachers:
                    teacher_names = [str(t) for t in teachers if isinstance(t, str) and t]
                    if teacher_names:
                        output.append(f"   üë®‚Äçüè´ –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: {', '.join(teacher_names[:2])}")

                groups = lesson.get('groups', [])
                if isinstance(groups, list) and groups:
                    group_names = [str(g) for g in groups if g]
                    if group_names:
                        output.append(f"   üë• –ì—Ä—É–ø–ø—ã: {', '.join(group_names[:3])}")

                week = lesson.get('week', '')
                if week and week != '–Ω–µ —É–∫–∞–∑–∞–Ω–æ':
                    output.append(f"   üìÖ –ù–µ–¥–µ–ª—è: {week}")

        output.append("=" * 60)
        output.append(f"üìä –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(lessons)}")
        return "\n".join(output)

    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)
    def format_schedule_response(self, results: List[Dict]) -> str:
        if not results:
            return " –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                     "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°—É–±–±–æ—Ç–∞": 6, "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": 7}

        def sort_key(result):
            chunk = result["payload"]
            day_num = day_order.get(chunk.get("day", ""), 99)
            time_str = chunk.get("time", "")
            para_match = re.search(r'(\d+)', time_str)
            para_num = int(para_match.group(1)) if para_match else 99
            return (day_num, para_num, -result.get("score", 0))

        results_sorted = sorted(results, key=sort_key)
        output = ["üìÖ **–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ**"]
        current_day = None
        entry_number = 1
        for result in results_sorted:
            chunk = result["payload"]
            day = chunk.get("day", "")
            if day != current_day:
                current_day = day
                output.append(f"\nüìÜ {current_day}:")
                entry_number = 1
            output.append(f"{entry_number}. **{chunk.get('time', '')}**")
            output.append(f"   üìö {chunk.get('subject', '')}")
            room = chunk.get('room', '')
            if room:
                output.append(f"   üè¢ –ê—É–¥–∏—Ç–æ—Ä–∏—è: {room}")
            teachers = chunk.get('teacher', [])
            if isinstance(teachers, list):
                teacher_names = [item for i, item in enumerate(teachers) if i % 2 == 0 and isinstance(item, str)]
                if teacher_names:
                    output.append(f"   üë®‚Äçüè´ –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: {', '.join(teacher_names)}")
            groups = chunk.get('groups', [])
            if isinstance(groups, list):
                clean_groups = [match.group(1) for group_item in groups if
                                isinstance(group_item, str) and (match := re.search(r'(\d{3,4}[–∞-—è–º–∫]?)', group_item))]
                if clean_groups:
                    output.append(f"   üë• –ì—Ä—É–ø–ø—ã: {', '.join(clean_groups)}")
            week = chunk.get('week', '')
            if week and week != "–Ω–µ —É–∫–∞–∑–∞–Ω–æ":
                output.append(f"   üìÖ –ù–µ–¥–µ–ª—è: {week}")
            output.append("")
            entry_number += 1
        output.append("=" * 60)
        output.append(f"üìä –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(results_sorted)}")
        return "\n".join(output)

    def format_general_response(self, results: List[Dict]) -> str:
        if not results:
            return " –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
        output = ["üìö **–ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:**", "=" * 60]
        for i, result in enumerate(results[:5], 1):
            text = result["text"]
            preview = text[:200] + "..." if len(text) > 200 else text
            output.append(f"\n{i}. [—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']:.3f}]")
            output.append(f"   {preview}")
        return "\n".join(output)

    def search_documents_rag(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ú–µ—Ç–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ–±–µ–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö (–¥–ª—è RAG)"""
        query_vector = self.model.encode(query, normalize_embeddings=True).tolist()
        all_results = []
        seen_texts = set()

        for coll in [self.schedule_collection, self.text_collection]:
            try:
                results = self.qdrant.search(
                    collection_name=coll,
                    query_vector=query_vector,
                    limit=top_k,
                    with_payload=True,
                )

                for item in results:
                    text = item.payload.get("full_text", item.payload.get("text", ""))
                    if not text or text in seen_texts:
                        continue

                    seen_texts.add(text)
                    all_results.append({
                        "id": item.id,
                        "score": float(item.score),
                        "text": text,
                        "collection": coll,
                    })
            except Exception:
                continue

        all_results.sort(key=lambda x: x["score"], reverse=True)
        return all_results[:top_k]

    def build_context(self, documents: List[Dict]) -> str:
        """–ú–µ—Ç–æ–¥ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        context_parts = []
        for i, doc in enumerate(documents):
            clean_text = re.sub(r"<\[document\]>|\[document\]>", "", doc["text"])
            clean_text = re.sub(r"\s+", " ", clean_text).strip()[:1000]
            context_parts.append(f"[–î–æ–∫—É–º–µ–Ω—Ç {i + 1}]: {clean_text}")
        return "\n".join(context_parts)

    def generate_llm_answer(self, question: str, context: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.has_llm:
            return "LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞."
        return self.llm.generate_answer(question, context)

    def _process_general_with_llm(self, query: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å LLM"""
        docs = self.search_documents_rag(query, top_k=8)
        context = self.build_context(docs)
        llm_answer = self.llm.generate_answer(query, context)

        return {
            "query": query,
            "type": "general_llm",
            "results_count": len(docs),
            "formatted_results": f"ü§ñ –û–¢–í–ï–¢:\n{llm_answer}\n\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(docs)}",
            "message": f"–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
        }

    # –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥
    def process_query(self, query: str, use_llm_for_general: bool = True) -> Dict[str, Any]:
        analysis = self.detect_query_type(query)

        print(f"üîç –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {analysis}")

        # –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–ü–†–û–°–û–í –†–ê–°–ü–ò–°–ê–ù–ò–Ø
        if analysis["type"] == "schedule":
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
            has_real_criteria = any([
                analysis["groups"],
                analysis["rooms"],
                analysis["teachers"],
                analysis["days"],
                analysis["times"],
            ])

            if has_real_criteria:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
                lessons = self.search_schedule_flexible(query, analysis, limit=300)
                formatted_results = self.format_schedule_from_lessons(lessons)

                if lessons:
                    message = f"–ù–∞–π–¥–µ–Ω–æ {len(lessons)} –∑–∞–Ω—è—Ç–∏–π"
                else:
                    message = "–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

                return {
                    "query": query,
                    "type": "schedule",
                    "results_count": len(lessons),
                    "formatted_results": formatted_results,
                    "message": message,
                }
            else:
                # –ù–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –Ω–æ –µ—Å—Ç—å —Å–ª–æ–≤–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
                # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å –æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–∏
                if use_llm_for_general and self.has_llm:
                    return self._process_general_with_llm(query)
                else:
                    return {
                        "query": query,
                        "type": "general",
                        "results_count": 0,
                        "formatted_results": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –∑–∞–ø—Ä–æ—Å. –ù–∞–ø—Ä–∏–º–µ—Ä: '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã 4318'",
                        "message": "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞",
                    }

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        else:
            if use_llm_for_general and self.has_llm:
                return self._process_general_with_llm(query)
            else:
                target_collection = self.text_collection
                all_results = self.search_documents(query, target_collection, top_k=10)
                filtered_results = sorted(all_results, key=lambda x: x["score"], reverse=True)[:5]
                results_count = len(filtered_results)
                formatted_results = self.format_general_response(filtered_results)

                return {
                    "query": query,
                    "type": "general",
                    "results_count": results_count,
                    "formatted_results": formatted_results,
                    "message": f"–ù–∞–π–¥–µ–Ω–æ {results_count} –∑–∞–ø–∏—Å–µ–π" if results_count > 0 else "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                }


class LLMGenerator:
    def __init__(
            self,
            provider: str = "caila",
            api_key: Optional[str] = None,
            model: str = "gpt-4o-mini",
            temperature: float = 0.1,
    ):
        if provider != "caila":
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider}")

        self.provider = provider
        self.model = model
        self.temperature = temperature
        self.api_key = api_key or os.getenv("CAILA_API_KEY")

        if not self.api_key:
            raise ValueError("CAILA API key not provided")

        self.base_url = "https://caila.io/api/mlpgate/account/just-ai/model/openai-proxy/predict-with-config"

    def generate_answer(self, question: str, context: str) -> str:
        prompt = f"""–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–≥–æ –±–æ—Ç–∞. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
        –í–æ–ø—Ä–æ—Å: {question}
        –î–∞–π —Ç–æ—á–Ω—ã–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."""

        headers = {
            "MLP-API-KEY": self.api_key,
            "Content-Type": "application/json; charset=utf-8",
        }

        payload = {
            "data": {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a helpful university assistant."},
                    {"role": "user", "content": prompt},
                ],
            },
            "config": {
                "temperature": self.temperature,
                "max_tokens": 1000,
            },
        }

        try:
            body = json.dumps(payload, ensure_ascii=False).encode("utf-8")
            response = requests.post(
                self.base_url,
                headers=headers,
                data=body,
                timeout=60,
                proxies={"http": None, "https": None},
            )

            if response.status_code != 200:
                return f"–û—à–∏–±–∫–∞ API: {response.status_code}"

            data = response.json()
            if "choices" in data:
                return data["choices"][0]["message"]["content"]
            elif "data" in data and "choices" in data["data"]:
                return data["data"]["choices"][0]["message"]["content"]
            else:
                return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò"

        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"


if __name__ == "__main__":
    bot = UniversityBot(
        qdrant_url="http://212.192.220.24:6333",
        api_key="pii5z%cE1",
        llm_api_key="1000097868.198240.pKeMJ9397Eh0C2Ish703JfH2InBrylvoVg5cKHX1"
    )

    test_queries = [
        "4318 —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ",  # –ì—Ä—É–ø–ø–∞
        "–∞—É–¥ 52-17",  # –ê—É–¥–∏—Ç–æ—Ä–∏—è
        "—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –†–∞—Å–∫–æ–ø–∏–Ω–∞",  # –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å
        "–ë–æ–∂–µ–Ω–∫–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –ø–Ω",
        "3 –ø–∞—Ä–∞ 4318",  # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∏–ø–µ–Ω–¥–∏—é?",  # –û–±—â–∏–π –≤–æ–ø—Ä–æ—Å
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è?",  # –û–±—â–∏–π –≤–æ–ø—Ä–æ—Å
        "–ö–æ–≥–¥–∞ –∑–∏–º–Ω—è—è —Å–µ—Å—Å–∏—è?",  # –û–±—â–∏–π –≤–æ–ø—Ä–æ—Å
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"–¢–ï–°–¢ {i}: {query}")
        print(f"{'=' * 70}")

        result = bot.process_query(query, use_llm_for_general=True)

        print(f"–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {result['type']}")
        print(f"–ù–∞–π–¥–µ–Ω–æ: {result['results_count']}")
        print(f"\n{result['formatted_results']}")
        print(f"\nüí° {result['message']}")