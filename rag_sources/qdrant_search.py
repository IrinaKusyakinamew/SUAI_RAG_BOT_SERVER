from qdrant_client import QdrantClient, models
from qdrant_client.models import Filter, FieldCondition, MatchAny, MatchText
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Set, Optional
import re
import json
import os
import requests

DOC_PREFIX = {
    "group": "groups_",
    "room": "classrooms_",
    "teacher": "teachers_",
}


def extract_metadata(payload: dict) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç metadata –∏–∑ payload"""
    metadata = payload.get("metadata", {})
    if isinstance(metadata, str):
        try:
            metadata = json.loads(metadata)
        except:
            metadata = {}
    result = payload.copy()
    result.update(metadata)
    if "metadata" in result:
        del result["metadata"]
    return result


class UniversityBot:
    def __init__(self, qdrant_url: str, api_key: str, llm_api_key: str = None):
        embed_model = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        print("Loading embedding model...")
        self.model = SentenceTransformer(embed_model, device="cpu")
        print("Model loaded")
        self.qdrant = QdrantClient(url=qdrant_url, api_key=api_key, timeout=30, prefer_grpc=False)
        print("Qdrant client initialized")
        self.text_collection = "text_embeddings"
        self.schedule_collection = "schedules_embeddings"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        self._check_collections()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –¥–ª—è –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        if llm_api_key:
            self.llm = LLMGenerator(
                provider="caila",
                api_key=llm_api_key,
                model="gpt-4o-mini",
                temperature=0.1
            )
            self.has_llm = True
        else:
            self.has_llm = False
            print("LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")

    def _check_collections(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏–π"""
        try:
            collections = self.qdrant.get_collections()
            for coll in [self.text_collection, self.schedule_collection]:
                found = False
                for collection in collections.collections:
                    if collection.name == coll:
                        info = self.qdrant.get_collection(coll)
                        print(f"‚úì –ö–æ–ª–ª–µ–∫—Ü–∏—è '{coll}': {info.points_count} –∑–∞–ø–∏—Å–µ–π")
                        found = True
                        break
                if not found:
                    print(f"‚ö† –ö–æ–ª–ª–µ–∫—Ü–∏—è '{coll}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {e}")

    # ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–°–ü–ò–°–ê–ù–ò–Ø ==========

    def detect_query_type(self, query: str) -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞: —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –∏–ª–∏ –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å"""
        original_query = query
        query_lower = query.lower().strip()

        analysis = {
            "type": "general",
            "is_schedule": False,
            "groups": [],
            "rooms": [],
            "teachers": [],
            "days": [],
            "times": [],
            "original_query": original_query,
        }

        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        schedule_keywords = ["—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ", "–ø–∞—Ä–∞", "–ø–∞—Ä—ã", "–∞—É–¥–∏—Ç–æ—Ä–∏—è", "–∞—É–¥",
                             "–ª–µ–∫—Ü–∏—è", "–∑–∞–Ω—è—Ç–∏–µ", "—Å–µ–º–∏–Ω–∞—Ä", "–ø—Ä–∞–∫—Ç–∏–∫–∞"]
        has_schedule_kw = any(kw in query_lower for kw in schedule_keywords)

        # 2. –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        # –ì—Ä—É–ø–ø—ã
        group_matches = re.findall(r'\b\d{3,4}[–∞-—è–º–∫]?\b', query_lower)
        if group_matches:
            analysis["groups"] = list(set(group_matches))

        # –ê—É–¥–∏—Ç–æ—Ä–∏–∏ (–Ω–æ–º–µ—Ä–∞ —Å –¥–µ—Ñ–∏—Å–æ–º)
        room_matches = re.findall(r'\b\d+-\d+\b', query)
        if room_matches:
            analysis["rooms"] = room_matches

        # –î–Ω–∏ –Ω–µ–¥–µ–ª–∏
        days = ["–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫", "—Å—Ä–µ–¥–∞", "—á–µ—Ç–≤–µ—Ä–≥", "–ø—è—Ç–Ω–∏—Ü–∞", "—Å—É–±–±–æ—Ç–∞"]
        for day in days:
            if day in query_lower:
                analysis["days"].append(day.capitalize())

        # –ü–∞—Ä—ã
        for i in range(1, 7):
            if f"{i} –ø–∞—Ä–∞" in query_lower or f"{i}-—è –ø–∞—Ä–∞" in query_lower:
                analysis["times"].append(f"{i} –ø–∞—Ä–∞")

        # 3. –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏
        if has_schedule_kw or analysis["groups"] or analysis["rooms"] or analysis["days"] or analysis["times"]:
            words = original_query.split()
            for word in words:
                clean_word = re.sub(r'[.,!?;:]', '', word)
                if (len(clean_word) > 2 and
                        clean_word[0].isupper() and
                        not clean_word.isdigit() and
                        clean_word.lower() not in days and
                        clean_word.lower() not in schedule_keywords):
                    analysis["teachers"].append(clean_word)

        analysis["teachers"] = list(set(analysis["teachers"]))

        # 4. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        has_schedule_entities = any([
            analysis["groups"],
            analysis["rooms"],
            analysis["days"],
            analysis["times"],
            analysis["teachers"],
        ])

        if has_schedule_kw or has_schedule_entities:
            analysis["is_schedule"] = True
            analysis["type"] = "schedule"
        else:
            analysis["type"] = "general"

        return analysis

    def search_schedule_flexible(self, query: str, criteria: Dict[str, Any], limit=1000):
        """–ü–æ–∏—Å–∫ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è"""
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        if any([criteria["groups"], criteria["rooms"], criteria["teachers"],
                criteria["days"], criteria["times"]]):
            return self._search_schedule_with_filters(criteria, limit)

        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –Ω–æ –∑–∞–ø—Ä–æ—Å —è–≤–Ω–æ –æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–∏
        query_vector = self.model.encode(query, normalize_embeddings=True).tolist()

        try:
            results = self.qdrant.search(
                collection_name=self.schedule_collection,
                query_vector=query_vector,
                limit=limit,
                with_payload=True,
            )

            lessons = []
            seen_keys = set()

            for point in results:
                payload = point.payload or {}
                doc_id = payload.get('document_id', '')

                if not doc_id.startswith(('groups_', 'classrooms_', 'teachers_')):
                    continue

                metadata = extract_metadata(payload)

                day = metadata.get("day", "")
                time = metadata.get("time", "")
                subject = metadata.get("subject", "")
                week = metadata.get("week", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ")

                key = f"{day}|{time}|{subject}|{week}"

                if key not in seen_keys:
                    seen_keys.add(key)
                    lessons.append({
                        "day": day,
                        "time": time,
                        "subject": subject,
                        "week": week,
                        "room": metadata.get("room", ""),
                        "teacher": metadata.get("teacher", []),
                        "groups": metadata.get("groups", []),
                        "score": float(point.score),
                    })

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                         "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°—É–±–±–æ—Ç–∞": 6}
            time_order = {"1 –ø–∞—Ä–∞": 1, "2 –ø–∞—Ä–∞": 2, "3 –ø–∞—Ä–∞": 3,
                          "4 –ø–∞—Ä–∞": 4, "5 –ø–∞—Ä–∞": 5, "6 –ø–∞—Ä–∞": 6}

            lessons.sort(key=lambda x: (
                day_order.get(x["day"], 99),
                time_order.get(x["time"], 99),
                -x.get("score", 0)
            ))

            return lessons

        except Exception:
            return []

    def _search_schedule_with_filters(self, criteria: Dict[str, Any], limit=1000):
        """–ü–æ–∏—Å–∫ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
        filters = []

        if criteria.get("groups"):
            for group in criteria["groups"]:
                filters.append(
                    FieldCondition(key="metadata.groups", match=MatchAny(any=[group]))
                )

        if criteria.get("rooms"):
            for room in criteria["rooms"]:
                clean_value = re.sub(r'^(–∞—É–¥\.?|–∞—É–¥–∏—Ç–æ—Ä–∏—è|–∞—É–¥)\s*', '', room.lower()).strip()
                search_patterns = [clean_value]
                if '-' in clean_value:
                    search_patterns.append(clean_value.replace('-', ''))
                search_patterns.append(f" {clean_value} ")

                room_conditions = []
                for pattern in search_patterns:
                    room_conditions.append(
                        FieldCondition(key="metadata.room", match=MatchText(text=pattern))
                    )

                if room_conditions:
                    if len(room_conditions) > 1:
                        filters.append(Filter(should=room_conditions))
                    else:
                        filters.append(room_conditions[0])

        if criteria.get("teachers"):
            for teacher in criteria["teachers"]:
                filters.append(
                    FieldCondition(key="metadata.teacher", match=MatchText(text=teacher))
                )

        if criteria.get("days"):
            for day in criteria["days"]:
                filters.append(
                    FieldCondition(key="metadata.day", match=MatchText(text=day))
                )

        if criteria.get("times"):
            for time in criteria["times"]:
                filters.append(
                    FieldCondition(key="metadata.time", match=MatchText(text=time))
                )

        if not filters:
            return []

        filter_ = Filter(must=filters)

        try:
            all_points = []
            next_offset = None
            total_scanned = 0

            while total_scanned < limit:
                try:
                    scroll_result = self.qdrant.scroll(
                        collection_name=self.schedule_collection,
                        scroll_filter=filter_,
                        limit=500,
                        offset=next_offset,
                        with_payload=True
                    )

                    if not scroll_result or not scroll_result[0]:
                        break

                    points, next_offset = scroll_result
                    all_points.extend(points)
                    total_scanned += len(points)

                    if next_offset is None:
                        break

                except Exception:
                    break

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            lessons = []
            seen_keys = set()

            for point in all_points:
                payload = point.payload or {}
                doc_id = payload.get('document_id', '')

                if not doc_id.startswith(('groups_', 'classrooms_', 'teachers_')):
                    continue

                metadata = extract_metadata(payload)

                day = metadata.get("day", "")
                time = metadata.get("time", "")
                subject = metadata.get("subject", "")
                week = metadata.get("week", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ")

                key = f"{day}|{time}|{subject}|{week}"

                if key not in seen_keys:
                    seen_keys.add(key)
                    lessons.append({
                        "day": day,
                        "time": time,
                        "subject": subject,
                        "week": week,
                        "room": metadata.get("room", ""),
                        "teacher": metadata.get("teacher", []),
                        "groups": metadata.get("groups", []),
                    })

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                         "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°–∞–±–±–æ—Ç–∞": 6}
            time_order = {"1 –ø–∞—Ä–∞": 1, "2 –ø–∞—Ä–∞": 2, "3 –ø–∞—Ä–∞": 3,
                          "4 –ø–∞—Ä–∞": 4, "5 –ø–∞—Ä–∞": 5, "6 –ø–∞—Ä–∞": 6}

            lessons.sort(key=lambda x: (day_order.get(x["day"], 99), time_order.get(x["time"], 99)))

            return lessons

        except Exception:
            return []

    def format_schedule_from_lessons(self, lessons: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ"""
        if not lessons:
            return " –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        day_order = {"–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": 1, "–í—Ç–æ—Ä–Ω–∏–∫": 2, "–°—Ä–µ–¥–∞": 3,
                     "–ß–µ—Ç–≤–µ—Ä–≥": 4, "–ü—è—Ç–Ω–∏—Ü–∞": 5, "–°—É–±–±–æ—Ç–∞": 6, "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": 7}

        lessons_by_day = {}
        for lesson in lessons:
            day = lesson.get("day", "")
            if day not in lessons_by_day:
                lessons_by_day[day] = []
            lessons_by_day[day].append(lesson)

        output = ["üìÖ **–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ**"]

        for day in sorted(lessons_by_day.keys(), key=lambda x: day_order.get(x, 99)):
            output.append(f"\nüìÜ {day}:")
            day_lessons = lessons_by_day[day]

            time_order = {"1 –ø–∞—Ä–∞": 1, "2 –ø–∞—Ä–∞": 2, "3 –ø–∞—Ä–∞": 3,
                          "4 –ø–∞—Ä–∞": 4, "5 –ø–∞—Ä–∞": 5, "6 –ø–∞—Ä–∞": 6}
            day_lessons.sort(key=lambda x: time_order.get(x.get("time", ""), 99))

            for i, lesson in enumerate(day_lessons, 1):
                output.append(f"\n{i}. **{lesson.get('time', '')}**")
                output.append(f"   üìö {lesson.get('subject', '')}")

                room = lesson.get('room', '')
                if room and room != '–Ω–µ —É–∫–∞–∑–∞–Ω–æ':
                    output.append(f"   üè¢ –ê—É–¥–∏—Ç–æ—Ä–∏—è: {room}")

                teachers = lesson.get('teacher', [])
                if isinstance(teachers, list) and teachers:
                    teacher_names = [str(t) for t in teachers if isinstance(t, str) and t]
                    if teacher_names:
                        output.append(f"   üë®‚Äçüè´ –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: {', '.join(teacher_names[:2])}")

                groups = lesson.get('groups', [])
                if isinstance(groups, list) and groups:
                    group_names = [str(g) for g in groups if g]
                    if group_names:
                        output.append(f"   üë• –ì—Ä—É–ø–ø—ã: {', '.join(group_names[:3])}")

                week = lesson.get('week', '')
                if week and week != '–Ω–µ —É–∫–∞–∑–∞–Ω–æ':
                    output.append(f"   üìÖ –ù–µ–¥–µ–ª—è: {week}")

        output.append("=" * 60)
        output.append(f"üìä –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(lessons)}")
        return "\n".join(output)

    # ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–ë–©–ò–• –í–û–ü–†–û–°–û–í (—Å—Ç–∞—Ä–∞—è —Ä–∞–±–æ—Ç–∞—é—â–∞—è –≤–µ—Ä—Å–∏—è) ==========

    def search_documents(self, query: str, top_k: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å—Ç–∞—Ä–∞—è —Ä–∞–±–æ—Ç–∞—é—â–∞—è –≤–µ—Ä—Å–∏—è)"""
        query_vector = self.model.encode(query, normalize_embeddings=True).tolist()

        all_results = []
        seen_texts: Set[str] = set()

        # –ò—â–µ–º –≤ –æ–±–µ–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö
        for coll in [self.text_collection, self.schedule_collection]:
            try:
                results = self.qdrant.search(
                    collection_name=coll,
                    query_vector=query_vector,
                    limit=top_k,
                    with_payload=True,
                )

                for item in results:
                    text = item.payload.get("text", "")
                    if not text or text in seen_texts:
                        continue

                    seen_texts.add(text)
                    all_results.append({
                        "id": item.id,
                        "score": float(item.score),
                        "text": text,
                        "collection": coll,
                        "metadata": {
                            k: v for k, v in item.payload.items() if k != "text"
                        },
                    })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{coll}': {e}")
                continue

        all_results.sort(key=lambda x: x["score"], reverse=True)
        return all_results[:top_k]

    def build_context(self, documents: List[Dict]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å—Ç–∞—Ä–∞—è —Ä–∞–±–æ—Ç–∞—é—â–∞—è –≤–µ—Ä—Å–∏—è)"""
        context_parts = []

        for i, doc in enumerate(documents):
            clean_text = re.sub(r"<\[document\]>|\[document\]>", "", doc["text"])
            clean_text = re.sub(r"\s+", " ", clean_text).strip()

            context_parts.append(
                f"[–î–æ–∫—É–º–µ–Ω—Ç {i + 1} | –∫–æ–ª–ª–µ–∫—Ü–∏—è: {doc['collection']} | "
                f"—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {doc['score']:.3f}]\n{clean_text}\n"
            )

        return "\n".join(context_parts)

    # ========== –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î –û–ë–†–ê–ë–û–¢–ö–ò ==========

    def process_query(self, query: str, use_llm_for_general: bool = True) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        analysis = self.detect_query_type(query)
        print(f"üîç –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {analysis}")

        # –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–ü–†–û–°–û–í –†–ê–°–ü–ò–°–ê–ù–ò–Ø
        if analysis["type"] == "schedule":
            lessons = self.search_schedule_flexible(query, analysis, limit=300)
            formatted_results = self.format_schedule_from_lessons(lessons)

            if lessons:
                message = f"–ù–∞–π–¥–µ–Ω–æ {len(lessons)} –∑–∞–Ω—è—Ç–∏–π"
            else:
                message = "–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

            return {
                "query": query,
                "type": "schedule",
                "results_count": len(lessons),
                "formatted_results": formatted_results,
                "message": message,
            }

        # –û–ë–†–ê–ë–û–¢–ö–ê –û–ë–©–ò–• –í–û–ü–†–û–°–û–í (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥ –∏–∑ UniversityRAGBot
            docs = self.search_documents(query, top_k=8)

            if not docs:
                return {
                    "query": query,
                    "type": "general",
                    "results_count": 0,
                    "formatted_results": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.",
                    "message": "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                }

            # –ï—Å–ª–∏ –µ—Å—Ç—å LLM –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            if use_llm_for_general and self.has_llm:
                context = self.build_context(docs)
                llm_answer = self.llm.generate_answer(query, context)

                return {
                    "query": query,
                    "type": "general_llm",
                    "results_count": len(docs),
                    "formatted_results": f"ü§ñ {llm_answer}\n\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(docs)}",
                    "message": f"–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                }

            # –ë–µ–∑ LLM - –ø—Ä–æ—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            else:
                output = ["üìö **–ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:**", "=" * 60]
                for i, doc in enumerate(docs[:5], 1):
                    text = doc["text"]
                    preview = text[:300] + "..." if len(text) > 300 else text
                    output.append(f"\n{i}. [—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {doc['score']:.3f}]")
                    output.append(f"   {preview}")

                formatted_response = "\n".join(output)

                return {
                    "query": query,
                    "type": "general",
                    "results_count": len(docs),
                    "formatted_results": formatted_response,
                    "message": f"–ù–∞–π–¥–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                }


class LLMGenerator:
    def __init__(
            self,
            provider: str = "caila",
            api_key: Optional[str] = None,
            model: str = "gpt-4o-mini",
            temperature: float = 0.1,
    ):
        if provider != "caila":
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider}")

        self.provider = provider
        self.model = model
        self.temperature = temperature

        self.api_key = api_key or os.getenv("CAILA_API_KEY")
        if not self.api_key:
            raise ValueError("CAILA API key not provided")

        self.author = "just-ai"
        self.service = "openai-proxy"
        self.base_url = (
            f"https://caila.io/api/mlpgate/account/"
            f"{self.author}/model/{self.service}/predict-with-config"
        )

    def generate_answer(self, question: str, context: str) -> str:
        """–°—Ç–∞—Ä—ã–π —Ä–∞–±–æ—Ç–∞—é—â–∏–π –ø—Ä–æ–º–ø—Ç –∏–∑ UniversityRAGBot"""
        prompt = f"""
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–≥–æ –±–æ—Ç–∞.
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ
–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –Ω—É–∂–Ω–æ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –¥–µ–∫–∞–Ω–∞—Ç –∏–ª–∏ –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–π –æ—Ç–¥–µ–ª.
–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞:
{question}

–î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
"""

        headers = {
            "MLP-API-KEY": self.api_key,
            "Content-Type": "application/json; charset=utf-8",
        }

        payload = {
            "data": {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful university assistant.",
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    },
                ],
            },
            "config": {
                "temperature": self.temperature,
                "max_tokens": 1000,
            },
        }

        try:
            body = json.dumps(payload, ensure_ascii=False).encode("utf-8")

            response = requests.post(
                self.base_url,
                headers=headers,
                data=body,
                timeout=60,
                proxies={
                    "http": None,
                    "https": None,
                },
            )

            if response.status_code != 200:
                return f"–û—à–∏–±–∫–∞ API {response.status_code}: {response.text}"

            data = response.json()

            if "choices" in data:
                return data["choices"][0]["message"]["content"]

            if "data" in data and "choices" in data["data"]:
                return data["data"]["choices"][0]["message"]["content"]

            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò"

        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {repr(e)}"