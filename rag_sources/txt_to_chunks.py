import os
import json
import re
import shutil
from pathlib import Path
import tiktoken
from tqdm import tqdm


# –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è (rag_sources)
PROJECT_DIR = Path(__file__).resolve().parent

# –ü–∞–ø–∫–∞, –≥–¥–µ –ª–µ–∂–∞—Ç txt —Ñ–∞–π–ª—ã
TXT_DIR = PROJECT_DIR / "schedules_txt"

# –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —á–∞–Ω–∫–æ–≤
TMP_DIR = PROJECT_DIR / "tmp_chunks"
TMP_DIR.mkdir(parents=True, exist_ok=True)

# –ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ TXT
OUTPUT_JSON = TMP_DIR / "chunks_txt.json"

CHUNK_SIZE = 512   # —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
CHUNK_OVERLAP = 50 # –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏

def normalize_text(text):
    return re.sub(r'\s+', ' ', text).strip()


def chunk_by_gpt_tokens(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
    tokens = enc.encode(text)

    chunks = []
    start = 0
    chunk_id = 0

    while start < len(tokens):
        end = min(start + chunk_size, len(tokens))
        chunk_text = enc.decode(tokens[start:end])

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ–±–µ–ª–∞, —á—Ç–æ–±—ã –Ω–µ —Ä–µ–∑–∞—Ç—å —Å–ª–æ–≤–∞
        last_space = chunk_text.rfind(" ")
        if last_space != -1 and end != len(tokens):
            trimmed = chunk_text[:last_space]
            new_end = start + len(enc.encode(trimmed))
            chunk_text = trimmed
        else:
            new_end = end

        chunks.append({
            "chunk_id": chunk_id,
            "chunk_uid": f"txt_chunk_{chunk_id}",
            "text": chunk_text.strip(),
            "token_count": len(enc.encode(chunk_text)),
        })

        # —Å–¥–≤–∏–≥–∞–µ–º start —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        start = max(new_end - overlap, new_end)
        chunk_id += 1

    return chunks


def process_all_txt():
    all_chunks = []
    global_id = 0

    txt_files = list(TXT_DIR.glob("*.txt"))

    if not txt_files:
        print("–ù–µ—Ç txt —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return

    for path in tqdm(txt_files, desc="Chunking TXT files"):
        with open(path, "r", encoding="utf-8") as f:
            text = f.read()

        text = normalize_text(text)
        chunks = chunk_by_gpt_tokens(text)

        for chunk in chunks:
            chunk["source_file"] = path.name
            chunk["global_id"] = global_id
            chunk["chunk_uid"] = f"{global_id}_txt"
            all_chunks.append(chunk)
            global_id += 1

    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump({"chunks": all_chunks}, f, ensure_ascii=False, indent=2)

    print(f"TXT —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_JSON}")

    # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ø–∞–ø–∫—É —Å txt —Ñ–∞–π–ª–∞–º–∏
    if TXT_DIR.exists():
        shutil.rmtree(TXT_DIR)
        print(f"üóë –ü–∞–ø–∫–∞ {TXT_DIR} —É–¥–∞–ª–µ–Ω–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è JSON")


if __name__ == "__main__":
    process_all_txt()
