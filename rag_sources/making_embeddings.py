import json
import torch
from tqdm import tqdm
from pathlib import Path
from loguru import logger
from langchain_huggingface import HuggingFaceEmbeddings
import warnings
import shutil

warnings.filterwarnings("ignore")

# –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞
BASE_DIR = Path(__file__).parent.parent

# –ü–∞–ø–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ JSON –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
TMP_JSON_DIR = BASE_DIR / "rag_sources" / "tmp_json_for_embedding"

# –ü–∞–ø–∫–∞ –¥–ª—è –∏—Ç–æ–≥–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
EMBEDDINGS_DIR = BASE_DIR / "rag_sources" / "embeddings"
EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)

# –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32

def load_chunks_from_folder(folder: Path):
    """–°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ JSON —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ"""
    all_chunks = []
    json_files = sorted(folder.glob("*.json"))
    if not json_files:
        logger.warning(f"–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ {folder.resolve()}")
        return all_chunks

    for file_path in json_files:
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –∏–∑ {file_path.name}")
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        chunks = data.get("chunks", data if isinstance(data, list) else [])
        all_chunks.extend(chunks)
    return all_chunks

def main():
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {MODEL_NAME} ({DEVICE})")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    embeddings_model = HuggingFaceEmbeddings(
        model_name=MODEL_NAME,
        model_kwargs={"device": DEVICE},
        encode_kwargs={"normalize_embeddings": True},
    )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ tmp_json_for_embedding
    chunks = load_chunks_from_folder(TMP_JSON_DIR)
    if not chunks:
        logger.error("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return

    logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

    output_file = EMBEDDINGS_DIR / "embeddings.jsonl"

    with open(output_file, "w", encoding="utf-8") as f_out:
        for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", ncols=100):
            batch = chunks[i:i + BATCH_SIZE]
            texts = [c["text"] for c in batch]
            uids = [c.get("chunk_uid", "") for c in batch]
            metadatas = [c.get("metadata", {}) for c in batch]

            try:
                batch_embeddings = embeddings_model.embed_documents(texts)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i}: {e}")
                continue

            for uid, text, emb, metadata in zip(uids, texts, batch_embeddings, metadatas):
                record = {
                    "id": uid,
                    "text": text,
                    "embedding": emb,
                    "metadata": metadata,
                }
                f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

    logger.success(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file.resolve()}")

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É —Å JSON –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    if TMP_JSON_DIR.exists():
        shutil.rmtree(TMP_JSON_DIR)
        logger.info(f"üóë –ü–∞–ø–∫–∞ {TMP_JSON_DIR} —É–¥–∞–ª–µ–Ω–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")


if __name__ == "__main__":
    main()
